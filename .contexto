# Documentação do Projeto de Análise de Dados Agrofit

## 1. Visão Geral do Projeto

O objetivo deste projeto é realizar uma análise interativa e em tempo real da base de dados pública **Agrofit – Produtos Formulados**. A solução foi migrada de um processo estático (baseado em scripts de ETL e dashboard em Looker Studio) para uma **aplicação web dinâmica construída com Streamlit**.

Esta nova abordagem conecta-se diretamente ao **Google BigQuery**, permitindo que as visualizações e análises sejam geradas sob demanda, garantindo escalabilidade e acesso aos dados mais recentes sem a necessidade de processamento manual ou uploads intermediários.

## 2. Arquitetura da Solução

A arquitetura atual é centrada em uma aplicação Python que utiliza o framework Streamlit para a interface do usuário e a biblioteca `google-cloud-bigquery` para comunicação com o banco de dados.

### 2.1. `dashboard_agrofit.py` - A Aplicação Central

Este script é o coração do projeto e consolida todas as funcionalidades:

- **Framework:** **Streamlit** é usado para criar a interface web interativa, organizar o layout em abas (tabs) e gerenciar o estado da aplicação.

- **Conectividade com o BigQuery:**
    - **Ação:** A aplicação se autentica no Google Cloud usando um arquivo de credenciais de Service Account (`gcp_credentials.json`).
    - **Performance:** As consultas ao BigQuery são otimizadas com o decorador `@st.cache_data`, que armazena em cache os resultados das queries. Isso reduz custos e melhora drasticamente a velocidade da aplicação, pois evita a re-execução de consultas idênticas a cada interação do usuário.

- **Visualização de Dados:**
    - **Biblioteca:** **Plotly Express** é utilizada para gerar todos os gráficos interativos (pizza, barras, mapas, etc.).
    - **Justificativa:** Plotly oferece uma vasta gama de gráficos esteticamente agradáveis e interativos, que permitem ao usuário explorar os dados de forma intuitiva (zoom, tooltips detalhados, etc.).

- **Estrutura da Interface:**
    - A aplicação é organizada em quatro abas temáticas, cada uma focada em um aspecto diferente da análise, guiando o usuário de uma visão macro para uma análise mais detalhada.

### 2.2. Google BigQuery como Data Warehouse

O BigQuery continua sendo a peça central para o armazenamento de dados, mas seu papel foi aprimorado:

- **Fonte de Dados em Tempo Real:** Em vez de ser apenas um repositório para um CSV processado, o BigQuery agora atua como um *live data source*. O Streamlit executa consultas SQL diretamente na tabela `tabela_agrofit_csv`.
- **Vantagens:**
    - **Escalabilidade:** O BigQuery processa as consultas de forma distribuída, lidando com grandes volumes de dados sem sobrecarregar a aplicação local.
    - **Dados Atualizados:** Qualquer atualização na tabela do BigQuery é refletida instantaneamente no dashboard, sem a necessidade de um novo ciclo de ETL.

### 2.3. Scripts Anteriores (`process_data.py` e `analyze_data.py`)

Os scripts `process_data.py` e `analyze_data.py` faziam parte da arquitetura anterior, que se baseava em um fluxo de ETL local. Embora o `process_data.py` contenha a lógica de limpeza que foi aplicada para gerar a tabela no BigQuery, eles **não são mais utilizados no fluxo de execução da aplicação Streamlit**. A análise e a visualização agora são feitas dinamicamente pela aplicação web.

## 3. Dockerização da Aplicação

Para facilitar o deploy e garantir um ambiente de execução consistente, a aplicação foi containerizada utilizando Docker.

### 3.1. `Dockerfile`

O `Dockerfile` define o ambiente da aplicação:

- **Imagem Base:** Utiliza uma imagem Python 3.10-slim, que é leve e otimizada.
- **Dependências:** Copia e instala as bibliotecas listadas no `requirements.txt`.
- **Código da Aplicação:** Adiciona os scripts Python necessários para a execução.
- **Exposição de Porta:** Expõe a porta 8501, padrão do Streamlit.
- **Comando de Execução:** Inicia a aplicação com `streamlit run dashboard_agrofit.py`.

### 3.2. `.dockerignore`

Este arquivo garante que arquivos desnecessários (como o repositório `.git`, credenciais e arquivos de documentação) não sejam copiados para a imagem Docker, mantendo-a segura e enxuta.